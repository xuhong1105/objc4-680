/*
 * Copyright (c) 2010-2012 Apple Inc. All rights reserved.
 *
 * @APPLE_LICENSE_HEADER_START@
 *
 * This file contains Original Code and/or Modifications of Original Code
 * as defined in and that are subject to the Apple Public Source License
 * Version 2.0 (the 'License'). You may not use this file except in
 * compliance with the License. Please obtain a copy of the License at
 * http://www.opensource.apple.com/apsl/ and read it before using this
 * file.
 *
 * The Original Code and all software distributed under the License are
 * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
 * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
 * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
 * Please see the License for the specific language governing rights and
 * limitations under the License.
 *
 * @APPLE_LICENSE_HEADER_END@
 */


/***********************************************************************
* Inlineable parts of NSObject / objc_object implementation
* NSObject / objc_object çš„å®ç°ä¸­çš„ å†…è”éƒ¨åˆ†
**********************************************************************/

#ifndef _OBJC_OBJCOBJECT_H_
#define _OBJC_OBJCOBJECT_H_


// åªå¼•ç”¨äº†è¿™ä¸€ä¸ªå¤´æ–‡ä»¶ï¼Œæ‰€ä»¥åªå’Œ objc-private æœ‰å…³ç³»
#include "objc-private.h"


enum ReturnDisposition : bool {
    ReturnAtPlus0 = false,
    ReturnAtPlus1 = true
};

static ALWAYS_INLINE 
bool prepareOptimizedReturn(ReturnDisposition disposition);


#if SUPPORT_TAGGED_POINTERS

#define TAG_COUNT 8
#define TAG_SLOT_MASK 0xf

#if SUPPORT_MSB_TAGGED_POINTERS
#   define TAG_MASK (1ULL<<63)
#   define TAG_SLOT_SHIFT 60
#   define TAG_PAYLOAD_LSHIFT 4
#   define TAG_PAYLOAD_RSHIFT 4
#else
#   define TAG_MASK 1
#   define TAG_SLOT_SHIFT 0
#   define TAG_PAYLOAD_LSHIFT 0
#   define TAG_PAYLOAD_RSHIFT 4
#endif

extern "C" { extern Class objc_debug_taggedpointer_classes[TAG_COUNT*2]; }
#define objc_tag_classes objc_debug_taggedpointer_classes

#endif


// åˆ¤æ–­ä¸€ä¸ª objc_object ç»“æ„ä½“å¯¹è±¡æ˜¯å¦æ˜¯ ç±»
inline bool
objc_object::isClass()
{
    // ä¾‹å¦‚ NSNumber ç­‰ TaggedPointer ä¸æ˜¯ç±»ï¼Œé‡Œé¢å­˜çš„å°±æ˜¯å€¼æœ¬èº«
    if (isTaggedPointer()) return false;
    
    // å…ˆå–å¾—è¿™ä¸ªç»“æ„ä½“å¯¹è±¡ä¸­å­˜å‚¨åœ¨ isa ä¸­çš„ clsï¼Œç„¶ååˆ¤æ–­è¿™ä¸ª cls æ˜¯å¦æ˜¯å…ƒç±»ï¼Œ
    // å¦‚æœæ˜¯å…ƒç±»é‚£ä¹ˆè¿™ä¸ªç»“æ„ä½“å¯¹è±¡å°±æ˜¯ç±»ï¼Œåä¹‹ä¸æ˜¯
    return ISA()->isMetaClass();
}

#if SUPPORT_NONPOINTER_ISA  // iphone çœŸæœºæ”¯æŒ

#   if !SUPPORT_TAGGED_POINTERS
#       error sorry
#   endif


inline Class 
objc_object::ISA() 
{
    assert(!isTaggedPointer());
    // å–å¾— isa.bits ä¸­çš„ shiftclsï¼Œå³åŸæ¥çš„ class cls
    return (Class)(isa.bits & ISA_MASK);
}


inline bool 
objc_object::hasIndexedIsa()
{
    // 0è¡¨ç¤ºæ™®é€šçš„isaæŒ‡é’ˆ 1è¡¨ç¤ºä¼˜åŒ–è¿‡çš„ï¼Œå­˜å‚¨å¼•ç”¨è®¡æ•°
    return isa.indexed;
}

inline Class 
objc_object::getIsa() 
{
    if (isTaggedPointer()) { // å¦‚æœæ˜¯ tagged pointerï¼Œéœ€è¦è½¬æ¢åå–å‡ºé‡Œé¢çš„class
        uintptr_t slot = ((uintptr_t)this >> TAG_SLOT_SHIFT) & TAG_SLOT_MASK;
        return objc_tag_classes[slot];
    }
    return ISA(); // å¦åˆ™ç›´æ¥è¿”å› isa_t isa ä¸­å­˜çš„ cls
}


inline void 
objc_object::initIsa(Class cls)
{
    initIsa(cls, false, false);
}

inline void 
objc_object::initClassIsa(Class cls)
{
    // disable non-pointer isa fields
    if (DisableIndexedIsa) {
        initIsa(cls, false, false);
    } else {
        initIsa(cls, true, false);
    }
}

inline void
objc_object::initProtocolIsa(Class cls)
{
    return initClassIsa(cls);
}

inline void 
objc_object::initInstanceIsa(Class cls, bool hasCxxDtor)
{
    assert(!UseGC);
    // å¦‚æœéœ€è¦ raw isa ï¼Œå°±ç©ä¸ä¸‹å»äº†
    assert(!cls->requiresRawIsa());
    assert(hasCxxDtor == cls->hasCxxDtor());

    initIsa(cls, true, hasCxxDtor);
}

// cls : ç±»
// indexed : whether enable non-pointer isa fields
// hasCxxDtor : æ˜¯å¦æœ‰c++çš„ææ„å‡½æ•°
inline void 
objc_object::initIsa(Class cls, bool indexed, bool hasCxxDtor)
{ 
    assert(!isTaggedPointer()); 
    
    if (!indexed) {
        isa.cls = cls;
    } else {
        // å¦‚æœç”¨ non-pointer isa fields
        // å°±ä¼šåœ¨å…¶ä¸­å­˜å…¶ä»–çš„ä¸œè¥¿
        assert(!DisableIndexedIsa);
        isa.bits = ISA_MAGIC_VALUE;
        // isa.magic is part of ISA_MAGIC_VALUE
        // isa.indexed is part of ISA_MAGIC_VALUE
        isa.has_cxx_dtor = hasCxxDtor;
        // ä¸ºä»€ä¹ˆæ˜¯å³ç§»3ä½ï¼Ÿï¼Ÿï¼Ÿ
        isa.shiftcls = (uintptr_t)cls >> 3;
    }
}

// ä¿®æ”¹ä¸€ä¸ª objc_object å¯¹è±¡çš„ clsï¼Œåªæ˜¯ clsï¼Œä¸æ˜¯æ•´ä¸ª isa
// æ–¹æ³•åå« changeIsa ï¼Œæ˜¯å› ä¸º objc.h ä¸­æš´éœ²å‡ºæ¥çš„å‡ objc_object é‡Œå£°æ˜ Class isa
// è¿™æ ·å¯ä»¥éšè— isa çš„å®ç°ç»†èŠ‚
inline Class 
objc_object::changeIsa(Class newCls)
{
    // This is almost always true but there are
    // enough edge cases that we can't assert it.
    // assert(newCls->isFuture()  || 
    //        newCls->isInitializing()  ||  newCls->isInitialized());

    assert(!isTaggedPointer()); 

    isa_t oldisa;
    isa_t newisa;

    bool sideTableLocked = false;
    // transcribe è½¬å½• æŠ„å†™
    bool transcribeToSideTable = false;

    do {
        transcribeToSideTable = false;
        oldisa = LoadExclusive(&isa.bits);
        if ((oldisa.bits == 0  ||  oldisa.indexed)  &&
            !newCls->isFuture()  &&  newCls->canAllocIndexed())
        {
            // 0 -> indexed
            // indexed -> indexed
            if (oldisa.bits == 0) {
                newisa.bits = ISA_MAGIC_VALUE;
            } else {
                newisa = oldisa;
            }
            // isa.magic is part of ISA_MAGIC_VALUE
            // isa.indexed is part of ISA_MAGIC_VALUE
            newisa.has_cxx_dtor = newCls->hasCxxDtor();
            newisa.shiftcls = (uintptr_t)newCls >> 3;
        }
        else if (oldisa.indexed) {
            // indexed -> not indexed
            // Need to copy retain count et al to side table.
            // Acquire side table lock before setting isa to 
            // prevent races such as concurrent -release.
            if (!sideTableLocked) sidetable_lock();
            sideTableLocked = true;
            transcribeToSideTable = true;
            newisa.cls = newCls;
        }
        else {
            // not indexed -> not indexed
            newisa.cls = newCls;
        }
    } while (!StoreExclusive(&isa.bits, oldisa.bits, newisa.bits));

    // ä» indexed å˜ä¸º not indexedï¼Œisa_t é‡Œä¸èƒ½å†å­˜å¼•ç”¨è®¡æ•°
    // åˆ™å°† oldisa ä¸­çš„ä¿¡æ¯ç§»åˆ° side table ä¸­
    if (transcribeToSideTable) {
        // Copy oldisa's retain count et al to side table.
        // oldisa.weakly_referenced: nothing to do
        // oldisa.has_assoc: nothing to do
        // oldisa.has_cxx_dtor: nothing to do
        sidetable_moveExtraRC_nolock(oldisa.extra_rc, 
                                     oldisa.deallocating, 
                                     oldisa.weakly_referenced);
    }

    if (sideTableLocked) sidetable_unlock();

    Class oldCls;
    if (oldisa.indexed) {
        // å·¦ç§» 3 ä½ï¼Œå–å‡ºcls
        oldCls = (Class)((uintptr_t)oldisa.shiftcls << 3);
    } else {
        oldCls = oldisa.cls;
    }
    
    return oldCls;
}

// SUPPORT_TAGGED_POINTERS
// åˆ¤æ–­å½“å‰å¯¹è±¡æ˜¯å¦æ˜¯ tagged pointer
inline bool 
objc_object::isTaggedPointer() 
{
    return ((uintptr_t)this & TAG_MASK);
}

    
inline bool
objc_object::hasAssociatedObjects()
{
    if (isTaggedPointer()) {
        return true;
    }
    if (isa.indexed) {
        return isa.has_assoc;
    }
    return true;
}


inline void
objc_object::setHasAssociatedObjects()
{
    if (isTaggedPointer()) return;

 retry:
    // LoadExclusive é‡Œåšäº†å¾ˆç„å¦™çš„äº‹æƒ…ï¼Œå®Œå…¨çœ‹ä¸æ‡‚ï¼Œåº”è¯¥ä¸ç”¨æ·±ç©¶
    isa_t oldisa = LoadExclusive(&isa.bits);
    isa_t newisa = oldisa;
    if (!newisa.indexed) return;
    if (newisa.has_assoc) return;
    newisa.has_assoc = true;
    if (!StoreExclusive(&isa.bits, oldisa.bits, newisa.bits)) goto retry;
}


inline bool
objc_object::isWeaklyReferenced()
{
    assert(!isTaggedPointer());
    if (isa.indexed) {
        return isa.weakly_referenced;
    } else {
        return sidetable_isWeaklyReferenced();
    }
}

// è®¾ç½®æœ‰å¼±å¼•ç”¨
inline void
objc_object::setWeaklyReferenced_nolock()
{
 retry:
    isa_t oldisa = LoadExclusive(&isa.bits);
    isa_t newisa = oldisa;
    // isaä¸­æ²¡æœ‰indexedï¼Œåˆ™å¼•ç”¨è®¡æ•°æ˜¯ç”±side tableç®¡ç†çš„
    // å¿…é¡»åœ¨side tableä¸­è®¾ç½®æœ‰å¼±å¼•ç”¨
    if (!newisa.indexed) {
        return sidetable_setWeaklyReferenced_nolock();
    }
    // isaä¸­åŸæœ¬å·²ç»æ˜¯æœ‰å¼±å¼•ç”¨ï¼Œåˆ™ä¸ç”¨ä¿®æ”¹
    if (newisa.weakly_referenced) return;
    // åŸæ¥æ²¡æœ‰å¼±å¼•ç”¨ï¼Œåˆ™ä¿®æ”¹ä¸ºæœ‰å¼±å¼•ç”¨
    newisa.weakly_referenced = true;
    // ä¿®æ”¹æŒ‡å®šåœ°å€çš„å€¼
    // è¿™é‡Œæ˜¯å°†isa.bitsä½ç½®å¤„çš„å€¼ï¼Œç”±oldisa.bitsä¿®æ”¹ä¸ºnewisa.bits
    // StoreExclusive é‡Œç”¨äº†__sync_bool_compare_and_swapå†…å»ºå‡½æ•°ï¼Œå…ˆæ¯”è¾ƒåäº¤æ¢ï¼Œå¦‚æœä¸­é—´å¤±è´¥äº†ï¼Œå°±ä¼šè¿”å›falseã€‚
    // é‚£ä¹ˆå°±é‡æ–°å†è¯•ä¸€æ¬¡.....è¿˜çœŸæ˜¯ç²—æš´å‘¢.....
    if (!StoreExclusive(&isa.bits, oldisa.bits, newisa.bits)) {
        goto retry;
    }
}


inline bool
objc_object::hasCxxDtor()
{
    assert(!isTaggedPointer());
    // å¯¹è±¡æ˜¯å¦æœ‰c++ææ„å™¨ï¼Œæˆ–è€…å¯¹è±¡çš„ç±»æ˜¯å¦æœ‰c++ææ„å™¨
    if (isa.indexed) {
        return isa.has_cxx_dtor;
    } else {
        return isa.cls->hasCxxDtor();
    }
}


// æ˜¯å¦åœ¨ deallocating
inline bool 
objc_object::rootIsDeallocating()
{
    // å¦‚æœæœ‰ Garbage Collectionï¼Œè¿™äº›æ“ä½œéƒ½æ˜¯éæ³•çš„ï¼Œæ‰€ä»¥è¦æ–­è¨€
    assert(!UseGC);

    if (isTaggedPointer()) {
        return false;
    }
    if (isa.indexed) {
        return isa.deallocating;
    }
    return sidetable_isDeallocating();
}

// æ¸…ç©ºå¼•ç”¨è®¡æ•°è¡¨å¹¶æ¸…é™¤å¼±å¼•ç”¨è¡¨ï¼Œå°†æ‰€æœ‰weakå¼•ç”¨æŒ‡nilï¼ˆè¿™ä¹Ÿå°±æ˜¯weakå˜é‡èƒ½å®‰å…¨ç½®ç©ºçš„æ‰€åœ¨ï¼‰
inline void 
objc_object::clearDeallocating()
{
    // å¦‚æœå¼•ç”¨è®¡æ•°éƒ½å­˜åœ¨äº† side table ä¸­ï¼Œé‚£ä¹ˆç›´æ¥æ“ä½œ side table å°±å¥½äº†
    if (!isa.indexed) {
        // Slow path for raw pointer isa.
        sidetable_clearDeallocating();
    }
    
    // å¦‚æœå¯¹è±¡æœ‰è¢«å¼±å¼•ç”¨ï¼Œæˆ–è€…æœ‰éƒ¨åˆ†å¼•ç”¨è®¡æ•°å­˜åœ¨äº† side table ä¸­ï¼Œ
    // å°±è°ƒç”¨ clearDeallocating_slow 
    else if (isa.weakly_referenced  ||  isa.has_sidetable_rc) {
        // Slow path for non-pointer isa with weak refs and/or side table data.
        clearDeallocating_slow();
    }

    assert(!sidetable_present());
}


inline void
objc_object::rootDealloc()
{
    assert(!UseGC);
    // ç›´æ¥è¿”å›äº†ï¼Ÿ tagged point å°±ä¸éœ€è¦æ¸…ç†äº†ï¼Ÿ
    if (isTaggedPointer()) return;

    if (isa.indexed  &&  
        !isa.weakly_referenced  &&  
        !isa.has_assoc  &&  
        !isa.has_cxx_dtor  &&  
        !isa.has_sidetable_rc)
    {
        // æ²¡æœ‰å¼±å¼•ç”¨ã€å…³è”å¯¹è±¡ã€c++ææ„å™¨ï¼Œå¼•ç”¨è®¡æ•°è¿˜ä¸å­˜å‚¨åœ¨side tableçš„è¯
        // å°±ç›´æ¥ free æ‰å¯¹è±¡çš„å†…å­˜
        // çœŸç²—æš´ğŸ˜‚ğŸ˜‚ğŸ˜‚ï¼Œéš¾æ€ªè¯´å¯ä»¥å¤§å¹…åº¦æé«˜æ•ˆç‡
        assert(!sidetable_present());
        free(this);
    } 
    else {
        // object_dispose ä¸­ä¼šåšè°ƒç”¨c++ææ„å™¨ã€æ¸…é™¤å…³è”å¯¹è±¡ã€æ¸…é™¤å¼±å¼•ç”¨ã€é‡Šæ”¾å†…å­˜ç­‰ç­‰å·¥ä½œï¼ŒæŠŠå¯¹è±¡å®‰å…¨å½»åº•çš„å¹²æ‰
        object_dispose((id)this);
    }
}


// Equivalent to calling [this retain], with shortcuts if there is no override
inline id 
objc_object::retain()
{
    // UseGC is allowed here, but requires hasCustomRR.
    // æœ‰è‡ªå®šä¹‰ RR (ä¸çŸ¥é“æ˜¯ä»€ä¹ˆç©æ„å„¿)ï¼Œå°±å¯ä»¥ç”¨ GC
    // åæ­£ GC è¿™ç©æ„å„¿åœ¨iOSä¸Šä¹Ÿä¸èƒ½ç”¨ï¼Œå°±ä¸å»ç®¡å®ƒäº†
    assert(!UseGC  ||  ISA()->hasCustomRR());
    assert(!isTaggedPointer());

    // æ²¡æœ‰è‡ªå®šä¹‰ retain/release
    if (! ISA()->hasCustomRR()) {
        return rootRetain();
    }

    // æœ‰è‡ªå®šä¹‰ retain/release
    // çŒœçš„ï¼Œæœ€åä¼šè°ƒç”¨ objc_retainï¼Œè§ fixupMessageRef
    return ((id(*)(objc_object *, SEL))objc_msgSend)(this, SEL_retain);
}


// Base retain implementation, ignoring overrides.
// This does not check isa.fast_rr; if there is an RR override then 
// it was already called and it chose to call [super retain].
//
// tryRetain=true is the -_tryRetain path.
// handleOverflow=false is the frameless fast path.
// handleOverflow=true is the framed slow path including overflow to side table
// The code is structured this way to prevent duplication.

ALWAYS_INLINE id 
objc_object::rootRetain()
{
    return rootRetain(false, false);
}

ALWAYS_INLINE bool 
objc_object::rootTryRetain()
{
    return rootRetain(true, false) ? true : false;
}

#pragma mark - rootRetain

// çœ‹åå­—å°±çŸ¥é“ï¼Œè‚¯å®šæ˜¯æœ€æ ¹æœ¬çš„ retain äº†
ALWAYS_INLINE id 
objc_object::rootRetain(bool tryRetain, bool handleOverflow)
{
    assert(!UseGC);
    // tagged pointer ä¸éœ€è¦ retain
    if (isTaggedPointer()) return (id)this;

    bool sideTableLocked = false;
    bool transcribeToSideTable = false;

    isa_t oldisa;
    isa_t newisa;

    do {
        transcribeToSideTable = false;
        oldisa = LoadExclusive(&isa.bits);
        newisa = oldisa;
        if (!newisa.indexed) goto unindexed;
        // don't check newisa.fast_rr; we already called any RR overrides
        // æ­£åœ¨ææ„ï¼Œå°±ä¸èƒ½ retain äº†ï¼Œå¾ˆæœ‰é“ç†
        if (tryRetain && newisa.deallocating) {
            goto tryfail;
        }
        uintptr_t carry;
        // å‘ newisa.bits ä¸­åŠ  RC_ONE
        // RC_ONE æ˜¯ä¸€ä¸ªå¼•ç”¨è®¡æ•° RC  retain count
        // carry å¯èƒ½æ˜¯æ£€æµ‹æ˜¯å¦æº¢å‡ºçš„
        newisa.bits = addc(newisa.bits, RC_ONE, 0, &carry);  // extra_rc++

        if (carry) { // å¦‚æœæº¢å‡ºäº†
            // newisa.extra_rc++ overflowed
            // ä¸å¤„ç†æº¢å‡º
            if (!handleOverflow) {
                return rootRetain_overflow(tryRetain);
            }
            // Leave half of the retain counts inline and 
            // prepare to copy the other half to the side table.
            // å¤„ç†æº¢å‡ºï¼Œç•™ä¸€åŠå¼•ç”¨è®¡æ•°åœ¨ isa.bits é‡Œ
            // å¦ä¸€åŠç§»åˆ° side table é‡Œ
            if (!tryRetain && !sideTableLocked) {
                // ç»™ side table åŠ é”ï¼Œå› ä¸ºåé¢è¦æ“ä½œå®ƒ
                sidetable_lock();
            }
            sideTableLocked = true;
            // æ ‡è®°è¦ç§»åŠ¨å¼•ç”¨è®¡æ•°åˆ° sidetable
            transcribeToSideTable = true;
            // ç•™ä¸€åŠå¼•ç”¨è®¡æ•°åœ¨ isa.bits é‡Œ
            newisa.extra_rc = RC_HALF;
            // åœ¨ isa ä¸­æ ‡è®°æœ‰éƒ¨åˆ†å¼•ç”¨è®¡æ•°åœ¨ side table ä¸­
            newisa.has_sidetable_rc = true;
        }
    } while (!StoreExclusive(&isa.bits, oldisa.bits, newisa.bits));// ä¸€ç›´äº¤æ¢ï¼Œäº¤æ¢åˆ°æˆåŠŸä¸ºæ­¢ï¼Œå°±æ˜¯è¿™ä¹ˆç²—æš´

    // å¦‚æœéœ€è¦ç§»åŠ¨å¼•ç”¨è®¡æ•°
    if (transcribeToSideTable) {
        // Copy the other half of the retain counts to the side table.
        // ç»™ side table é‡Œå­˜çš„å¼•ç”¨è®¡æ•°åŠ  RC_HALF
        sidetable_addExtraRC_nolock(RC_HALF);
    }

    // æå®šåï¼Œç»™ side table è§£é”
    if (!tryRetain && sideTableLocked) sidetable_unlock();
    return (id)this;

 tryfail:
    if (!tryRetain && sideTableLocked) sidetable_unlock();
    return nil;

 unindexed: // æ²¡æœ‰ indexed ï¼Œå¼•ç”¨è®¡æ•°éƒ½åœ¨side tableï¼Œç›´æ¥æ“ä½œside tableå°±å¥½äº†
    if (!tryRetain && sideTableLocked) {
        sidetable_unlock();
    }
    // æ˜¯å¦ try retain ï¼Œè°ƒç”¨çš„ side table çš„æ–¹æ³•è¿˜ä¸ä¸€æ ·
    if (tryRetain) {
        return sidetable_tryRetain() ? (id)this : nil;
    } else {
        return sidetable_retain();
    }
}


// Equivalent to calling [this release], with shortcuts if there is no override
inline void
objc_object::release()
{
    // UseGC is allowed here, but requires hasCustomRR.
    assert(!UseGC  ||  ISA()->hasCustomRR());
    assert(!isTaggedPointer());

    // æ²¡æœ‰è‡ªå®šä¹‰ retain/release
    if (! ISA()->hasCustomRR()) {
        rootRelease();
        return;
    }

    // æœ‰è‡ªå®šä¹‰ RR
    ((void(*)(objc_object *, SEL))objc_msgSend)(this, SEL_release);
}


// Base release implementation, ignoring overrides.
// Does not call -dealloc.
// Returns true if the object should now be deallocated.
// This does not check isa.fast_rr; if there is an RR override then
// it was already called and it chose to call [super release].
// 
// handleUnderflow=false is the frameless fast path.
// handleUnderflow=true is the framed slow path including side table borrow
// The code is structured this way to prevent duplication.

ALWAYS_INLINE bool
objc_object::rootRelease()
{
    return rootRelease(true, false);
}

ALWAYS_INLINE bool 
objc_object::rootReleaseShouldDealloc()
{
    return rootRelease(false, false);
}

// å¦‚æœå¯¹è±¡å¼•ç”¨è®¡æ•°ä¸º0ï¼Œåˆ™éœ€è¦è¢«dealloc å°±è¿”å› trueï¼Œå¦åˆ™è¿”å› false
// ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è‹¥å¼•ç”¨è®¡æ•°ç­‰äº0ï¼Œæ˜¯å¦æ‰§è¡Œdeallocï¼Œä½†æ˜¯æ— è®ºæ‰§è¡Œdeallocï¼Œéƒ½ä¼šè¿”å›true
ALWAYS_INLINE bool 
objc_object::rootRelease(bool performDealloc, bool handleUnderflow)
{
    assert(!UseGC);
    // tagged pointer ä¸éœ€è¦ retain release
    if (isTaggedPointer()) return false;

    bool sideTableLocked = false;

    isa_t oldisa;
    isa_t newisa;

 retry:
    do {
        oldisa = LoadExclusive(&isa.bits);
        newisa = oldisa;
        // å¦‚æœæ²¡æœ‰ indexed ï¼Œç›´æ¥æ“ä½œ side table
        if (!newisa.indexed) {
            goto unindexed;
        }
        // don't check newisa.fast_rr; we already called any RR overrides
        uintptr_t carry;
        // isa ä¸­å­˜çš„å¼•ç”¨è®¡æ•°å‡ä¸€
        newisa.bits = subc(newisa.bits, RC_ONE, 0, &carry);  // extra_rc--
        // å¦‚æœæº¢å‡ºäº†ï¼ˆå‘ä¸‹æº¢å‡ºï¼‰
        if (carry) {
            goto underflow;
        }
    } while (!StoreReleaseExclusive(&isa.bits, oldisa.bits, newisa.bits));

    if (sideTableLocked) sidetable_unlock();
    // å¼•ç”¨è®¡æ•°è¿˜å¤§äº0ï¼Œä¸éœ€è¦deallocï¼Œæ‰€ä»¥è¿”å›false
    return false;

 underflow:
    // newisa.extra_rc-- underflowed: borrow from side table or deallocate

    // abandon newisa to undo the decrement
    // è®© newisa é‡æ–°ç­‰äº oldisaï¼Œå›åˆ°å‡ä¸€å‰çš„çŠ¶æ€
    newisa = oldisa;

    // å¦‚æœæœ‰éƒ¨åˆ†å¼•ç”¨è®¡æ•°å­˜åœ¨äº† side table ä¸­
    if (newisa.has_sidetable_rc) {
        
        // ä¸å¤„ç†æº¢å‡º
        if (!handleUnderflow) {
            // å‘è´§ rootRelease_underflow é‡Œè¿˜æ˜¯è°ƒç”¨ rootRelease æ–¹æ³•
            // åªæ˜¯ç¬¬äºŒä¸ªå‚æ•°å˜æˆäº† true rootRelease(performDealloc, true);
            // é‚£ä¹ˆç»ˆç©¶è¿˜æ˜¯å¤„ç†äº†æº¢å‡ºï¼Œå§æ§½ï¼Œä»€ä¹ˆé€»è¾‘
            return rootRelease_underflow(performDealloc);
        }

        // Transfer retain count from side table to inline storage.

        if (!sideTableLocked) {
            sidetable_lock();
            sideTableLocked = true;
            if (!isa.indexed) {
                // Lost a race vs the indexed -> not indexed transition
                // before we got the side table lock. Stop now to avoid 
                // breaking the safety checks in the sidetable ExtraRC code.
                goto unindexed;
            }
        }

        // Try to remove some retain counts from the side table.
        // ä» side table å‡æ‰å¤§å°ä¸º RC_HALF çš„å¼•ç”¨è®¡æ•°
        size_t borrowed = sidetable_subExtraRC_nolock(RC_HALF);

        // To avoid races, has_sidetable_rc must remain set 
        // even if the side table count is now zero.

        // ä¸ºé¿å… races (ç«äº‰ï¼Ÿå•¥ç©æ„å„¿)ï¼Œå³ä½¿ç°åœ¨ side table é‡Œå­˜çš„å¼•ç”¨è®¡æ•°æ˜¯0ï¼Œhas_sidetable_rc ä¹Ÿå¿…é¡»ä¿æŒ 1
        
        // è‹¥åŸæ¥side tableé‡Œå­˜æœ‰å¼•ç”¨è®¡æ•°ï¼Œé‚£ä¹ˆborrowedåº”è¯¥ç­‰äºRC_HALFï¼ŒçŒœçš„
        // è‹¥ borrowed < 0ï¼Œside table é‡Œè‚¯å®šæ²¡å­˜å¼•ç”¨è®¡æ•°
        if (borrowed > 0) {
            // Side table retain count decreased.
            // Try to add them to the inline count.
            
            // release æ“ä½œï¼Œæ‰€ä»¥éœ€è¦å‡ä¸€
            newisa.extra_rc = borrowed - 1;  // redo the original decrement too
            // å‘ isa.bits é‡Œè£…è½½æ–°çš„ newisa.bits
            bool stored = StoreExclusive(&isa.bits, oldisa.bits, newisa.bits);
            
            // å¦‚æœå¤±è´¥äº†ï¼Œç«‹å³æ¢ addc æ–¹æ³•é‡æ–°å°è¯•
            if (!stored) {
                // Inline update failed. 
                // Try it again right now. This prevents livelock(æ´»é”ï¼Ÿ) on LL/SC
                // architectures where the side table access itself may have 
                // dropped the reservation.
                isa_t oldisa2 = LoadExclusive(&isa.bits);
                isa_t newisa2 = oldisa2;
                if (newisa2.indexed) {
                    uintptr_t overflow;
                    // å‘ newisa2.bits ä¸€æ¬¡åŠ  borrowed-1 ä¸ªå¼•ç”¨è®¡æ•°
                    // å¹¶ä¸”çœ‹æœ‰æ²¡æœ‰æº¢å‡º
                    newisa2.bits = 
                        addc(newisa2.bits, RC_ONE * (borrowed-1), 0, &overflow);
                    if (!overflow) {
                        // å¦‚æœæ²¡æœ‰æº¢å‡ºï¼Œå°†æ–°çš„ newisa2 è£…è¿› isa.bits é‡Œ
                        stored = StoreReleaseExclusive(&isa.bits, oldisa2.bits, newisa2.bits);
                    }
                }
            }

            // æ‰€æœ‰çš„åŠªåŠ›éƒ½å¤±è´¥äº†ï¼Œé‚£ä¹ˆå°±åªèƒ½å°†å¼•ç”¨è®¡æ•°å…ˆé‡æ–°æ”¾å› side tableï¼Œç„¶åå›åˆ° retryï¼Œå†é‡å¤´å°è¯•ä¸€æ¬¡ï¼Œå¦ˆè›‹ï¼ŒçœŸç®€å•ç²—æš´
            if (!stored) {
                // Inline update failed.
                // Put the retains back in the side table.
                sidetable_addExtraRC_nolock(borrowed);
                goto retry;
            }

            // Decrement successful after borrowing from side table.
            // This decrement cannot be the deallocating decrement - the side 
            // table lock and has_sidetable_rc bit ensure that if everyone 
            // else tried to -release while we worked, the last one would block.
            sidetable_unlock();
            
            // å› ä¸ºside tableé‡Œæœ‰å¼•ç”¨è®¡æ•°ï¼Œæ‰€ä»¥å¼•ç”¨è®¡æ•°è‚¯å®šå¤§äº 0ï¼Œæ‰€ä»¥ä¸éœ€è¦ dealloc
            return false;
        }
        else {
            // Side table is empty after all. Fall-through to the dealloc path.
            // side table å–å‡ºçš„å¼•ç”¨è®¡æ•°ç­‰äº 0ï¼Œå³è¿™ä¸ªå¯¹è±¡æ€»å…±çš„å¼•ç”¨è®¡æ•°æ˜¯ 0ï¼Œå¯¹è±¡å°±å¯ä»¥å¹²æ‰äº†ï¼Œå°±ä¼šèµ°åˆ°ä¸‹é¢çš„ dealloc
        }
    }

    // Really deallocate.
    // çœŸæ­£åœ¨ dealloc æ“ä½œ ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜

    if (sideTableLocked) sidetable_unlock();

    // å¦‚æœå·²ç»åœ¨ dealloc , å°±å°´å°¬äº†ï¼Œç›´æ¥æŠ¥é”™
    if (newisa.deallocating) {
        return overrelease_error();
    }
    
    // æ ‡è®°ä¸ºæ­£åœ¨ dealloc
    newisa.deallocating = true;
    
    // è£…è½½å¤±è´¥ï¼Œåˆè¦é‡æ–°æ¥ä¸€æ¬¡ï¼Œå•Šï¼Œå§æ§½
    if (!StoreExclusive(&isa.bits, oldisa.bits, newisa.bits)) goto retry;
    __sync_synchronize();
    if (performDealloc) {
        // æ‰§è¡Œ deallocï¼Œæœ€ç»ˆçš„ dealloc è¿˜æ˜¯ç”± SEL_dealloc å®ç°
        // SEL_dealloc é‡Œç©¶ç«Ÿå¹²äº†å•¥å‘¢
        ((void(*)(objc_object *, SEL))objc_msgSend)(this, SEL_dealloc);
    }
    return true;

 unindexed: // å¦‚æœå¼•ç”¨è®¡æ•°ç­‰ä¿¡æ¯éƒ½å­˜åœ¨äº† side table ä¸­ï¼Œå°±è°ƒç”¨ sidetable_release è¿›è¡Œ release
    if (sideTableLocked) sidetable_unlock();
    return sidetable_release(performDealloc);
}


// Equivalent to [this autorelease], with shortcuts if there is no override
inline id 
objc_object::autorelease()
{
    // UseGC is allowed here, but requires hasCustomRR.
    assert(!UseGC  ||  ISA()->hasCustomRR());

    if (isTaggedPointer()) {
        return (id)this;
    }
    if (! ISA()->hasCustomRR()) {
        return rootAutorelease();
    }

    return ((id(*)(objc_object *, SEL))objc_msgSend)(this, SEL_autorelease);
}


// Base autorelease implementation, ignoring overrides.
inline id 
objc_object::rootAutorelease()
{
    assert(!UseGC);

    if (isTaggedPointer()) return (id)this;
    // æ£€æµ‹æ˜¯å¦æ”¯æŒ Optimized Returnï¼ˆä¸çŸ¥é“å¹²å˜›ç”¨çš„ï¼‰
    if (prepareOptimizedReturn(ReturnAtPlus1)) return (id)this;

    // rootAutorelease2 é‡Œçš„æ“ä½œæ˜¯ï¼Œå°†å½“å‰å¯¹è±¡æ·»åŠ è¿›äº†å½“å‰çš„ autoreleasepage ä¸­
    return rootAutorelease2();
}

// å–å¾—å¯¹è±¡çš„æ‰€æœ‰å¼•ç”¨è®¡æ•°ï¼ŒåŒ…æ‹¬ isa ä¸­çš„å’Œ side table ä¸­çš„
inline uintptr_t 
objc_object::rootRetainCount()
{
    assert(!UseGC);
    if (isTaggedPointer()) return (uintptr_t)this;

    // side table åŠ é”
    sidetable_lock();
    isa_t bits = LoadExclusive(&isa.bits);
    // å¦‚æœ indexed ï¼Œåˆ™æœ‰ä¸€éƒ¨åˆ†å¼•ç”¨è®¡æ•°å­˜åœ¨äº† isa.bits ä¸­
    if (bits.indexed) {
        // çœ‹æ¥ extra_rc ä¸­å­˜çš„çœŸçš„æ˜¯å‡ä¸€åçš„å€¼
        uintptr_t rc = 1 + bits.extra_rc;
        // å¦‚æœside tableæœ‰å¼•ç”¨è®¡æ•°
        if (bits.has_sidetable_rc) {
            // åŠ ä¸Šside tableä¸­å­˜çš„å¼•ç”¨è®¡æ•°
            rc += sidetable_getExtraRC_nolock();
        }
        // è§£é”side tableåè¿”å›å¼•ç”¨è®¡æ•°
        sidetable_unlock();
        return rc;
    }

    // æ²¡æœ‰ indexï¼Œåˆ™å…¨éƒ¨å¼•ç”¨è®¡æ•°éƒ½åœ¨side tableä¸­
    // ç›´æ¥è¿”å›side tableä¸­å­˜çš„å¼•ç”¨è®¡æ•°
    sidetable_unlock();
    return sidetable_retainCount();
}


// SUPPORT_NONPOINTER_ISA
#else
// not SUPPORT_NONPOINTER_ISA

// not SUPPORT_NONPOINTER_ISA çš„æ—¶å€™ï¼Œå¼•ç”¨è®¡æ•°éƒ½ä¿å­˜åœ¨ side table ä¸­
// æ‰€ä»¥å°‘äº†å¾ˆå¤šåˆ¤æ–­é€»è¾‘ï¼Œä»£ç éƒ½å¾ˆç®€å•
    
inline Class 
objc_object::ISA() 
{
    assert(!isTaggedPointer());
    // ä¸æ”¯æŒSUPPORT_NONPOINTER_ISAçš„è¯ï¼Œ
    // isa_t ç»“æ„ä½“ä¸­åªå­˜äº† cls ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥å–ï¼Œä¸éœ€è¦åšä½è¿ç®—
    return isa.cls;
}


inline bool 
objc_object::hasIndexedIsa()
{
    return false;
}


inline Class 
objc_object::getIsa() 
{
#if SUPPORT_TAGGED_POINTERS
    if (isTaggedPointer()) {
        // æ£€éªŒå¯¹è±¡æ˜¯å¦æ˜¯ tagged pointer çš„ç®—æ³•
        uintptr_t slot = ((uintptr_t)this >> TAG_SLOT_SHIFT) & TAG_SLOT_MASK;
        return objc_tag_classes[slot];
    }
#endif
    return ISA();
}


inline void 
objc_object::initIsa(Class cls)
{
    assert(!isTaggedPointer()); 
    isa = (uintptr_t)cls; 
}


inline void 
objc_object::initClassIsa(Class cls)
{
    initIsa(cls);
}


inline void 
objc_object::initProtocolIsa(Class cls)
{
    initIsa(cls);
}


inline void 
objc_object::initInstanceIsa(Class cls, bool)
{
    initIsa(cls);
}


inline void 
objc_object::initIsa(Class cls, bool, bool)
{ 
    initIsa(cls);
}


inline Class 
objc_object::changeIsa(Class cls)
{
    // This is almost always rue but there are 
    // enough edge cases that we can't assert it.
    // assert(cls->isFuture()  ||  
    //        cls->isInitializing()  ||  cls->isInitialized());

    assert(!isTaggedPointer()); 
    
    isa_t oldisa, newisa;
    newisa.cls = cls;
    do {
        oldisa = LoadExclusive(&isa.bits);
    } while (!StoreExclusive(&isa.bits, oldisa.bits, newisa.bits));
    
    if (oldisa.cls  &&  oldisa.cls->instancesHaveAssociatedObjects()) {
        cls->setInstancesHaveAssociatedObjects();
    }
    
    return oldisa.cls;
}

    
inline bool 
objc_object::isTaggedPointer() 
{
#if SUPPORT_TAGGED_POINTERS
    // TAG_MASK çš„å®šä¹‰å°±åœ¨å½“å‰æ–‡ä»¶é‡Œ
    return ((uintptr_t)this & TAG_MASK);
#else
    return false;
#endif
}


inline bool
objc_object::hasAssociatedObjects()
{
    assert(!UseGC);

    return getIsa()->instancesHaveAssociatedObjects();
}


inline void
objc_object::setHasAssociatedObjects()
{
    assert(!UseGC);

    getIsa()->setInstancesHaveAssociatedObjects();
}


inline bool
objc_object::isWeaklyReferenced()
{
    assert(!isTaggedPointer());
    assert(!UseGC);

    return sidetable_isWeaklyReferenced();
}


inline void 
objc_object::setWeaklyReferenced_nolock()
{
    assert(!isTaggedPointer());
    assert(!UseGC);

    sidetable_setWeaklyReferenced_nolock();
}


inline bool
objc_object::hasCxxDtor()
{
    assert(!isTaggedPointer());
    return isa.cls->hasCxxDtor();
}


inline bool 
objc_object::rootIsDeallocating()
{
    assert(!UseGC);

    if (isTaggedPointer()) return false;
    return sidetable_isDeallocating();
}


inline void 
objc_object::clearDeallocating()
{
    sidetable_clearDeallocating();
}


inline void
objc_object::rootDealloc()
{
    if (isTaggedPointer()) return;
    object_dispose((id)this);
}


// Equivalent to calling [this retain], with shortcuts if there is no override
inline id 
objc_object::retain()
{
    // UseGC is allowed here, but requires hasCustomRR.
    assert(!UseGC  ||  ISA()->hasCustomRR());
    assert(!isTaggedPointer());

    if (! ISA()->hasCustomRR()) {
        return sidetable_retain();
    }

    return ((id(*)(objc_object *, SEL))objc_msgSend)(this, SEL_retain);
}


// Base retain implementation, ignoring overrides.
// This does not check isa.fast_rr; if there is an RR override then 
// it was already called and it chose to call [super retain].
inline id 
objc_object::rootRetain()
{
    assert(!UseGC);

    if (isTaggedPointer()) return (id)this;
    return sidetable_retain();
}


// Equivalent to calling [this release], with shortcuts if there is no override
inline void
objc_object::release()
{
    // UseGC is allowed here, but requires hasCustomRR.
    assert(!UseGC  ||  ISA()->hasCustomRR());
    assert(!isTaggedPointer());

    if (! ISA()->hasCustomRR()) {
        sidetable_release();
        return;
    }
    
    ((void(*)(objc_object *, SEL))objc_msgSend)(this, SEL_release);
}


// Base release implementation, ignoring overrides.
// Does not call -dealloc.
// Returns true if the object should now be deallocated.
// This does not check isa.fast_rr; if there is an RR override then 
// it was already called and it chose to call [super release].
    
// NOT SUPPORT_NONPOINTER_ISA ç‰ˆæœ¬çš„ rootRelease çœŸç®€å•å•Š
// å¼•ç”¨è®¡æ•°å…¨éƒ½å­˜åœ¨ side table é‡Œï¼Œéœ€è¦åšçš„é€»è¾‘å°±å°‘å¤ªå¤šäº†
inline bool 
objc_object::rootRelease()
{
    assert(!UseGC);

    if (isTaggedPointer()) return false;
    // å°†å…¶ä» side table ä¸­åˆ é™¤ï¼Œå¹¶ dealloc
    return sidetable_release(true);
}

inline bool 
objc_object::rootReleaseShouldDealloc()
{
    if (isTaggedPointer()) return false;
    // å°†å…¶ä» side table ä¸­åˆ é™¤ï¼Œä½†ä¸ dealloc
    return sidetable_release(false);
}


// Equivalent to [this autorelease], with shortcuts if there is no override
inline id 
objc_object::autorelease()
{
    // UseGC is allowed here, but requires hasCustomRR.
    assert(!UseGC  ||  ISA()->hasCustomRR());

    if (isTaggedPointer()) return (id)this;
    if (! ISA()->hasCustomRR()) return rootAutorelease();

    return ((id(*)(objc_object *, SEL))objc_msgSend)(this, SEL_autorelease);
}


// Base autorelease implementation, ignoring overrides.
inline id 
objc_object::rootAutorelease()
{
    assert(!UseGC);

    if (isTaggedPointer()) return (id)this;
    // Optimized adj.æœ€ä½³çš„ï¼Œå‡†å¤‡æœ€ä½³çš„è¿”å›ï¼Œçœ‹ä¸æ‡‚ä»€ä¹ˆé¬¼
    if (prepareOptimizedReturn(ReturnAtPlus1)) return (id)this;

    return rootAutorelease2();
}


// Base tryRetain implementation, ignoring overrides.
// This does not check isa.fast_rr; if there is an RR override then 
// it was already called and it chose to call [super _tryRetain].
inline bool 
objc_object::rootTryRetain()
{
    assert(!UseGC);

    if (isTaggedPointer()) return true;
    return sidetable_tryRetain();
}


inline uintptr_t 
objc_object::rootRetainCount()
{
    assert(!UseGC);

    if (isTaggedPointer()) return (uintptr_t)this;
    return sidetable_retainCount();
}


// not SUPPORT_NONPOINTER_ISA

#endif


#if SUPPORT_RETURN_AUTORELEASE

/***********************************************************************
  Fast handling of return through Cocoa's +0 autoreleasing conventionï¼ˆçº¦å®šã€ä¹ ä¿—ï¼‰.
  The caller and calleeï¼ˆè¢«è°ƒç”¨è€…ï¼‰ cooperate to keep the returned object
  out of the autorelease pool and eliminateï¼ˆæ¶ˆé™¤ï¼‰ redundantï¼ˆå¤šä½™çš„ï¼‰ retain/release pairs.

  An optimized callee looks at the caller's instructions following the 
  return. If the caller's instructions are also optimized then the callee 
  skips all retain count operations: no autorelease, no retain/autorelease.
  Instead it saves the result's current retain count (+0 or +1) in 
  thread-local storage. If the caller does not look optimized then 
  the callee performs autorelease or retain/autorelease as usual.

  An optimized caller looks at the thread-local storage. If the result 
  is set then it performs any retain or release needed to change the 
  result from the retain count left by the callee to the retain count 
  desired by the caller. Otherwise the caller assumes the result is 
  currently at +0 from an unoptimized callee and performs any retain 
  needed for that case.

  There are two optimized callees:
    objc_autoreleaseReturnValue
      result is currently +1. The unoptimized path autoreleases it.
    objc_retainAutoreleaseReturnValue
      result is currently +0. The unoptimized path retains and autoreleases it.

  There are two optimized callers:
    objc_retainAutoreleasedReturnValue
      caller wants the value at +1. The unoptimized path retains it.
    objc_unsafeClaimAutoreleasedReturnValue
      caller wants the value at +0 unsafely. The unoptimized path does nothing.

  Example:

    Callee:
      // compute ret at +1
      return objc_autoreleaseReturnValue(ret);
    
    Caller:
      ret = callee();
      ret = objc_retainAutoreleasedReturnValue(ret);
      // use ret at +1 here

    Callee sees the optimized caller, sets TLS, and leaves the result at +1.
    Caller sees the TLS, clears it, and accepts the result at +1 as-is.

  The callee's recognition of the optimized caller is architecture-dependent.
  i386 and x86_64: Callee looks for `mov rax, rdi` followed by a call or 
    jump instruction to objc_retainAutoreleasedReturnValue or 
    objc_unsafeClaimAutoreleasedReturnValue. 
  armv7: Callee looks for a magic nop `mov r7, r7` (frame pointer register). 
  arm64: Callee looks for a magic nop `mov x29, x29` (frame pointer register). 

  Tagged pointer objects do participate in the optimized return scheme, 
  because it saves message sends. They are not entered in the autorelease 
  pool in the unoptimized case.
**********************************************************************/

/*
    æ–¹æ³•è¿”å›å€¼æ—¶çš„ autorelease æœºåˆ¶
    
    é‚£ä¹ˆè¿™é‡Œæœ‰ä¸€ä¸ªé—®é¢˜ï¼šä¸ºä»€ä¹ˆæ–¹æ³•è¿”å›å€¼çš„æ—¶å€™éœ€è¦ç”¨åˆ° autorelease æœºåˆ¶å‘¢ï¼Ÿ
    
    è¿™æ¶‰åŠåˆ°ä¸¤ä¸ªè§’è‰²çš„é—®é¢˜ã€‚ä¸€ä¸ªè§’è‰²æ˜¯è°ƒç”¨æ–¹æ³•æ¥æ”¶è¿”å›å€¼çš„æ¥æ”¶æ–¹ã€‚å½“å‚æ•°è¢«ä½œä¸ºè¿”å›å€¼ return ä¹‹åï¼Œæ¥æ”¶æ–¹å¦‚æœè¦æ¥ç€ä½¿ç”¨å®ƒå°±éœ€è¦å¼ºå¼•ç”¨å®ƒï¼Œä½¿å®ƒ retainCount +1ï¼Œç”¨å®Œåå†æ¸…ç†ï¼Œä½¿å®ƒ retainCount -1ã€‚æœ‰æŒæœ‰å°±æœ‰æ¸…ç†ï¼Œè¿™æ˜¯æ¥æ”¶æ–¹çš„è´£ä»»ã€‚å¦ä¸€ä¸ªè§’è‰²å°±æ˜¯è¿”å›å¯¹è±¡çš„æ–¹æ³•ï¼Œå³æä¾›æ–¹ã€‚åœ¨æ–¹æ³•ä¸­åˆ›å»ºäº†å¯¹è±¡å¹¶ä½œä¸ºè¿”å›å€¼æ—¶ï¼Œä¸€æ–¹é¢ä½ åˆ›å»ºäº†è¿™ä¸ªå¯¹è±¡ä½ å°±å¾—è´Ÿè´£é‡Šæ”¾å®ƒï¼Œæœ‰åˆ›å»ºå°±æœ‰é‡Šæ”¾ï¼Œè¿™æ˜¯åˆ›å»ºè€…çš„è´£ä»»ã€‚å¦ä¸€æ–¹é¢ä½ å¾—ä¿è¯è¿”å›æ—¶å¯¹è±¡æ²¡è¢«é‡Šæ”¾ä»¥ä¾¿æ–¹æ³•å¤–çš„æ¥æ”¶æ–¹èƒ½æ‹¿åˆ°æœ‰æ•ˆçš„å¯¹è±¡ï¼Œå¦åˆ™ä½ è¿”å›çš„æ˜¯ nilï¼Œæœ‰ä½•æ„ä¹‰å‘¢ã€‚æ‰€ä»¥å°±éœ€è¦æ‰¾ä¸€ä¸ªåˆç†çš„æœºåˆ¶æ—¢èƒ½å»¶é•¿è¿™ä¸ªå¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸï¼Œåˆèƒ½ä¿è¯å¯¹å…¶é‡Šæ”¾ã€‚è¿™ä¸ªæœºåˆ¶å°±æ˜¯ autorelease æœºåˆ¶ã€‚
    
    å½“å¯¹è±¡ä½œä¸ºå‚æ•°ä»æ–¹æ³•è¿”å›æ—¶ï¼Œä¼šè¢«æ”¾åˆ°æ­£åœ¨ä½¿ç”¨çš„ Autorelease Pool ä¸­ï¼Œç”±è¿™ä¸ª Autorelease Pool å¼ºå¼•ç”¨è¿™ä¸ªå¯¹è±¡è€Œä¸æ˜¯ç«‹å³é‡Šæ”¾ï¼Œä»è€Œå»¶é•¿äº†å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸï¼ŒAutorelease Pool è‡ªå·±é”€æ¯çš„æ—¶å€™ä¼šæŠŠå®ƒé‡Œé¢çš„å¯¹è±¡éƒ½é¡ºæ‰‹æ¸…ç†æ‰ï¼Œä»è€Œä¿è¯äº†å¯¹è±¡ä¼šè¢«é‡Šæ”¾ã€‚ä½†æ˜¯è¿™é‡Œä¹Ÿå¼•å‡ºå¦ä¸€ä¸ªé—®é¢˜ï¼šæ—¢ç„¶ä¼šå»¶é•¿å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸåˆ° Autorelease Pool è¢«é”€æ¯çš„æ—¶å€™ï¼Œé‚£ä¹ˆ Autorelease Pool çš„ç”Ÿå‘½å‘¨æœŸæ˜¯å¤šä¹…å‘¢ï¼Ÿä¼šä¸ä¼šåœ¨ Autorelease Pool éƒ½é”€æ¯äº†ï¼Œæ¥æ”¶æ–¹è¿˜æ²¡æ¥æ”¶åˆ°å¯¹è±¡å‘¢ï¼Ÿ
    
    Autorelease Pool æ˜¯ä¸çº¿ç¨‹ä¸€ä¸€æ˜ å°„çš„ï¼Œè¿™å°±æ˜¯è¯´ä¸€ä¸ª autoreleased çš„å¯¹è±¡çš„å»¶è¿Ÿé‡Šæ”¾æ˜¯å‘ç”Ÿåœ¨å®ƒæ‰€åœ¨çš„ Autorelease Pool å¯¹åº”çš„çº¿ç¨‹ä¸Šçš„ã€‚å› æ­¤ï¼Œåœ¨æ–¹æ³•è¿”å›å€¼çš„è¿™ä¸ªåœºæ™¯ä¸­ï¼Œå¦‚æœ Autorelease Pool çš„ drain æ–¹æ³•æ²¡æœ‰åœ¨æ¥æ”¶æ–¹å’Œæä¾›æ–¹äº¤æ¥çš„è¿‡ç¨‹ä¸­è§¦å‘ï¼Œé‚£ä¹ˆ autoreleased å¯¹è±¡æ˜¯ä¸ä¼šè¢«é‡Šæ”¾çš„ï¼ˆé™¤éä¸¥é‡é”™ä¹±çš„ä½¿ç”¨çº¿ç¨‹ï¼‰ã€‚
    
    é€šå¸¸ï¼ŒAutorelease Pool çš„é”€æ¯ä¼šè¢«å®‰æ’åœ¨å¾ˆå¥½çš„æ—¶é—´ç‚¹ä¸Šï¼š
    
    Run Loop ä¼šåœ¨æ¯æ¬¡ loop åˆ°å°¾éƒ¨æ—¶é”€æ¯ Autorelease Poolã€‚
    GCD çš„ dispatched blocks ä¼šåœ¨ä¸€ä¸ª Autorelease Pool çš„ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œï¼Œè¿™ä¸ª Autorelease Pool ä¸æ—¶çš„å°±è¢«é”€æ¯äº†ï¼ˆä¾èµ–äºå®ç°ç»†èŠ‚ï¼‰ã€‚NSOperationQueue ä¹Ÿæ˜¯ç±»ä¼¼ã€‚
    å…¶ä»–çº¿ç¨‹åˆ™ä¼šå„è‡ªå¯¹ä»–ä»¬å¯¹åº”çš„ Autorelease Pool çš„ç”Ÿå‘½å‘¨æœŸè´Ÿè´£ã€‚
    è‡³æ­¤ï¼Œæˆ‘ä»¬çŸ¥é“äº†ä¸ºä½•æ–¹æ³•è¿”å›å€¼éœ€è¦ autorelease æœºåˆ¶ï¼Œä»¥åŠè¿™ä¸€æœºåˆ¶æ˜¯å¦‚ä½•ä¿éšœæ¥æ”¶æ–¹èƒ½ä»æä¾›æ–¹é‚£é‡Œè·å¾—ä¾ç„¶é²œæ´»çš„å¯¹è±¡ã€‚

    åœ¨ MRC æ—¶ä»£ï¼Œå½“æˆ‘ä»¬è‡ªå·±åˆ›å»ºäº†å¯¹è±¡å¹¶æŠŠå®ƒä½œä¸ºæ–¹æ³•çš„è¿”å›å€¼è¿”å›å‡ºå»æ—¶ï¼Œéœ€è¦æ‰‹åŠ¨è°ƒç”¨å¯¹è±¡çš„ autorelease æ–¹æ³•ï¼Œå¦‚ä¸ŠèŠ‚æ‰€è®²çš„åˆ©ç”¨ autorelease æœºåˆ¶æ­£ç¡®è¿”å›å¯¹è±¡ã€‚åˆ°äº† ARC æ—¶ä»£ï¼ŒARC éœ€è¦ä¿æŒå¯¹ MRC ä»£ç çš„å…¼å®¹ï¼Œè¿™å°±æ„å‘³ç€ MRC çš„å®ç°å’Œ ARC çš„å®ç°å¯ä»¥ç›¸äº’æ›¿æ¢ï¼Œè€Œå¯¹è±¡æ¥æ”¶æ–¹å’Œå¯¹è±¡æä¾›æ–¹æ— éœ€çŸ¥é“å¯¹æ–¹æ˜¯ MRC å®ç°è¿˜æ˜¯ ARC å®ç°ä¹Ÿèƒ½æ­£ç¡®å·¥ä½œã€‚æ¯”å¦‚ï¼Œå½“åŸºäº MRC å®ç°çš„ä»£ç è°ƒç”¨ä½ çš„ä¸€ä¸ª ARC å®ç°çš„æ–¹æ³•æ¥è·å–ä¸€ä¸ªå¯¹è±¡ï¼Œé‚£ä¹ˆä½ çš„æ–¹æ³•å¿…é¡»åŒæ ·é‡‡ç”¨ä¸Šæ–‡æ‰€è®²çš„ autorelease æœºåˆ¶æ¥è¿”å›å¯¹è±¡ä»¥ç¡®ä¿å¯¹è±¡æ¥æ”¶æ–¹èƒ½æ­£ç¡®è·å¾—å¯¹è±¡ã€‚æ‰€ä»¥ï¼Œå³ä½¿åœ¨ ARC æ¨¡å¼ä¸‹å¯¹è±¡çš„ autorelease æ–¹æ³•ä¸å†èƒ½è¢«æ˜¾ç¤ºè°ƒç”¨ï¼Œä½†æ˜¯ autorelease çš„æœºåˆ¶ä»ç„¶æ˜¯åœ¨é»˜é»˜çš„å·¥ä½œç€ï¼Œåªæ˜¯ç¼–è¯‘å™¨åœ¨å¸®ä½ å®è·µè¿™ä¸€æœºåˆ¶ã€‚
    
  ARC æå‡ºäº†å·§å¦™çš„è¿è¡Œæ—¶ä¼˜åŒ–æ–¹æ¡ˆæ¥è·³è¿‡ autorelease æœºåˆ¶ã€‚è¿™ä¸ªè¿‡ç¨‹æ˜¯è¿™æ ·çš„ï¼šå½“æ–¹æ³•çš„è°ƒç”¨æ–¹å’Œå®ç°æ–¹çš„ä»£ç éƒ½æ˜¯åŸºäº ARC å®ç°çš„æ—¶å€™ï¼Œåœ¨æ–¹æ³• return çš„æ—¶å€™ï¼ŒARC ä¼šè°ƒç”¨ objc_autoreleaseReturnValue() æ›¿ä»£å‰é¢è¯´çš„ autoreleaseã€‚åœ¨è°ƒç”¨æ–¹æŒæœ‰æ–¹æ³•è¿”å›å¯¹è±¡çš„æ—¶å€™ï¼ˆä¹Ÿå°±æ˜¯åš retain çš„æ—¶å€™ï¼‰ï¼ŒARC ä¼šè°ƒç”¨ objc_retainAutoreleasedReturnValue()ã€‚åœ¨è°ƒç”¨ objc_autoreleaseReturnValue() æ—¶ï¼Œå®ƒä¼šåœ¨æ ˆä¸ŠæŸ¥è¯¢ return address æ¥ç¡®å®š return value æ˜¯å¦ä¼šè¢«ä¼ ç»™ objc_retainAutoreleasedReturnValue()ã€‚å¦‚æœæ²¡ä¼ ï¼Œé‚£ä¹ˆå®ƒå°±ä¼šèµ°å‰æ–‡æ‰€è®²çš„ autorelease çš„è¿‡ç¨‹ã€‚å¦‚æœä¼ äº†ï¼ˆè¿™è¡¨æ˜è¿”å›å€¼èƒ½é¡ºåˆ©ä»æä¾›æ–¹äº¤æ¥ç»™æ¥æ”¶æ–¹ï¼‰ï¼Œé‚£ä¹ˆå®ƒå°±è·³è¿‡ autorelease å¹¶åŒæ—¶ä¿®æ”¹ return address æ¥è·³è¿‡ objc_retainAutoreleasedReturnValue()ï¼Œä»è€Œä¸€ä¸¾æ¶ˆé™¤äº† autorelease å’Œ retain çš„è¿‡ç¨‹ã€‚è¿™ä¸ªæ–¹æ¡ˆå¯ä»¥åœ¨ MRC-to-ARC è°ƒç”¨ã€ARC-to-ARC è°ƒç”¨ä»¥åŠ ARC-to-MRC è°ƒç”¨ä¸­æ­£ç¡®å·¥ä½œï¼Œå¹¶åœ¨ç¬¦åˆæ¡ä»¶çš„ä¸€äº› ARC-to-ARC è°ƒç”¨ä¸­æ¶ˆé™¤ autorelease æœºåˆ¶ã€‚
*/
    
# if __x86_64__

static ALWAYS_INLINE bool 
callerAcceptsOptimizedReturn(const void * const ra0)
{
    const uint8_t *ra1 = (const uint8_t *)ra0;
    const uint16_t *ra2;
    const uint32_t *ra4 = (const uint32_t *)ra1;
    const void **sym;

#define PREFER_GOTPCREL 0
#if PREFER_GOTPCREL
    // 48 89 c7    movq  %rax,%rdi
    // ff 15       callq *symbol@GOTPCREL(%rip)
    if (*ra4 != 0xffc78948) {
        return false;
    }
    if (ra1[4] != 0x15) {
        return false;
    }
    ra1 += 3;
#else
    // 48 89 c7    movq  %rax,%rdi
    // e8          callq symbol
    if (*ra4 != 0xe8c78948) {
        return false;
    }
    ra1 += (long)*(const int32_t *)(ra1 + 4) + 8l;
    ra2 = (const uint16_t *)ra1;
    // ff 25       jmpq *symbol@DYLDMAGIC(%rip)
    if (*ra2 != 0x25ff) {
        return false;
    }
#endif
    ra1 += 6l + (long)*(const int32_t *)(ra1 + 2);
    sym = (const void **)ra1;
    if (*sym != objc_retainAutoreleasedReturnValue  &&  
        *sym != objc_unsafeClaimAutoreleasedReturnValue) 
    {
        return false;
    }

    return true;
}

// __x86_64__
# elif __arm__

static ALWAYS_INLINE bool 
callerAcceptsOptimizedReturn(const void *ra)
{
    // if the low bit is set, we're returning to thumb mode
    if ((uintptr_t)ra & 1) {
        // 3f 46          mov r7, r7
        // we mask off the low bit via subtraction
        if (*(uint16_t *)((uint8_t *)ra - 1) == 0x463f) {
            return true;
        }
    } else {
        // 07 70 a0 e1    mov r7, r7
        if (*(uint32_t *)ra == 0xe1a07007) {
            return true;
        }
    }
    return false;
}

// __arm__
# elif __arm64__

// åˆ¤æ–­è°ƒç”¨è€…æ˜¯å¦èƒ½æ¥å—ä¼˜åŒ–çš„è¿”å›å€¼ï¼ˆé¿å¼€ autoreleaseï¼‰
static ALWAYS_INLINE bool 
callerAcceptsOptimizedReturn(const void *ra)
{
    // fd 03 1d aa    mov fp, fp
    if (*(uint32_t *)ra == 0xaa1d03fd) {
        return true;
    }
    return false;
}

// __arm64__
# elif __i386__  &&  TARGET_IPHONE_SIMULATOR

static inline bool 
callerAcceptsOptimizedReturn(const void *ra)
{
    return false;
}

// __i386__  &&  TARGET_IPHONE_SIMULATOR
# else

#warning unknown architecture

static ALWAYS_INLINE bool 
callerAcceptsOptimizedReturn(const void *ra)
{
    return false;
}

// unknown architecture
# endif


static ALWAYS_INLINE ReturnDisposition 
getReturnDisposition()
{
    return (ReturnDisposition)(uintptr_t)tls_get_direct(RETURN_DISPOSITION_KEY);
}


static ALWAYS_INLINE void 
setReturnDisposition(ReturnDisposition disposition)
{
    tls_set_direct(RETURN_DISPOSITION_KEY, (void*)(uintptr_t)disposition);
}


// Try to prepare for optimized return with the given disposition (+0 or +1).
// Returns true if the optimized path is successful.
// Otherwise the return value must be retained and/or autoreleased as usual.
    
// å‡†å¤‡ä¼˜åŒ–è¿”å›å€¼ï¼Œå¦‚æœæˆåŠŸäº†å°±è¿”å›true
// å¤±è´¥çš„è¯ï¼Œå°±è¿”å›falseï¼Œè¿”å›å€¼å°±åªèƒ½æŒ‰åŸæ¥çš„å¥—è·¯ - autorelease é˜²æ­¢å¯¹è±¡åœ¨è°ƒç”¨æ–¹æ‹¿åˆ°è¿”å›å€¼å‰å°±è¢«é‡Šæ”¾
static ALWAYS_INLINE bool 
prepareOptimizedReturn(ReturnDisposition disposition)
{
    assert(getReturnDisposition() == ReturnAtPlus0);

    if (callerAcceptsOptimizedReturn(__builtin_return_address(0))) {
        if (disposition) {
            setReturnDisposition(disposition);
        }
        return true;
    }

    return false;
}


// Try to accept an optimized return.
// Returns the disposition of the returned object (+0 or +1).
// An un-optimized return is +0.
static ALWAYS_INLINE ReturnDisposition 
acceptOptimizedReturn()
{
    ReturnDisposition disposition = getReturnDisposition();
    setReturnDisposition(ReturnAtPlus0);  // reset to the unoptimized state
    return disposition;
}


// SUPPORT_RETURN_AUTORELEASE
#else
// not SUPPORT_RETURN_AUTORELEASE


static ALWAYS_INLINE bool
prepareOptimizedReturn(ReturnDisposition disposition __unused)
{
    return false;
}


static ALWAYS_INLINE ReturnDisposition 
acceptOptimizedReturn()
{
    return ReturnAtPlus0;
}


// not SUPPORT_RETURN_AUTORELEASE
#endif


// _OBJC_OBJECT_H_
#endif
